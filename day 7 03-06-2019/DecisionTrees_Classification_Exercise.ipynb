{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecisionTrees_Classification_Exercise.ipynb","version":"0.3.2","provenance":[{"file_id":"1Otbqmf4vVuudt9jnJUMFRCCe9BhCAe12","timestamp":1559153797207}],"collapsed_sections":["Kuqt1xuh_MXB"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4XElc7XH_MWa","colab_type":"text"},"source":["# June 3: Classification With Decision Trees"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pWMzKVUNl58c"},"source":["## Data Loading\n","\n","[Iris Flower Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"i8AnzHBFj4qa","cellView":"both","colab":{}},"source":["#@title\n","from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris['data']\n","y = iris['target']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qc2olBEi_MWj","colab_type":"text"},"source":["## Data Exploration\n","#### Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_ukuORaklboT","scrolled":true,"colab":{}},"source":["iris.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0yJ49dN_MWt","colab_type":"text"},"source":["<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/mDC1KSt/petal-sepal.png\" alt=\"petal-sepal\" border=\"0\"></a>"]},{"cell_type":"markdown","metadata":{"id":"DElAeagq_MWv","colab_type":"text"},"source":["#### Target Labels"]},{"cell_type":"code","metadata":{"id":"yPLSXUgI_MWw","colab_type":"code","colab":{}},"source":["iris.target_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2G-fkQcY_MW1","colab_type":"text"},"source":["<a href=\"https://ibb.co/8sJhx0R\"><img src=\"https://i.ibb.co/h9kqdyn/iris.png\" alt=\"iris\" border=\"0\"></a>"]},{"cell_type":"markdown","metadata":{"id":"yWRXx4i__MW2","colab_type":"text"},"source":["#### Dataset Size"]},{"cell_type":"code","metadata":{"id":"bJsSMJ4Y_MW3","colab_type":"code","colab":{}},"source":["len(iris.data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmRIz0Lo_MW7","colab_type":"text"},"source":["#### Visualizations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mr6cEwwtlHP9","colab":{}},"source":["from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","colors = ['r', 'y', 'b']\n","fig, ax = plt.subplots(1, 4, figsize=(20,4))\n","cols = [[0, 1], [2, 3], [0, 2], [1, 3]]\n","\n","for n in range(len(ax)):\n","    for t in range(len(iris.target_names)):\n","        ax[n].scatter([iris.data[i, cols[n][0]] for i,v in enumerate(iris.target) if v==t], \\\n","                [iris.data[i, cols[n][1]] for i,v in enumerate(iris.target) if v==t], c=colors[t])\n","    ax[n].set_title('{} Vs {}'.format(iris.feature_names[cols[n][0]], iris.feature_names[cols[n][1]]))\n","    ax[n].set_xlabel(iris.feature_names[cols[n][0]])\n","    ax[n].set_ylabel(iris.feature_names[cols[n][1]])\n","\n","fig.legend(labels=['setosa', 'versicolor', 'virginica'], ncol=1, loc='right', fontsize=15)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kuqt1xuh_MXB","colab_type":"text"},"source":["## Modeling A Classifier\n","\n","### See [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) docs for parameter list, attributes etc. \n","\n","The most \"relevant\" parameters:\n","* **criterion** : _The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Default is \"gini\"._\n","\n","* **splitter** : _The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split. Default is \"best\"._\n","\n","* **max_depth** : _The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default is `None`._\n","\n","* **min_samples_split** : _The minimum number of samples required to split an internal node. Default = `2`._\n","\n","* **min_samples_leaf** : _The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min_samples_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Default is `1`_"]},{"cell_type":"markdown","metadata":{"id":"wlrHPGAJ4q1d","colab_type":"text"},"source":["## TO DO: Train/Test The Data"]},{"cell_type":"code","metadata":{"id":"TlrZ9RWL4rAd","colab_type":"code","colab":{}},"source":["# TO DO: Split the data into test (20%) and train sets.\n","############ Your code goes here ############\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmyfmYl-4rVM","colab_type":"text"},"source":["## TO DO: Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ACp3DUz3j4su","colab":{}},"source":["# TO DO: Train a DecisionTreeClassifier model on train set.\n","\n","# Parameters\n","criterion = 'gini'\n","splitter = 'best'\n","max_depth = None\n","min_samples_split = 2\n","min_samples_leaf = 1\n","\n","############ Your code goes here ############\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MiO0MuUy40Tn","colab_type":"text"},"source":["## TO DO: Testing"]},{"cell_type":"code","metadata":{"id":"cTCETMR_4zji","colab_type":"code","colab":{}},"source":["# TO DO: Test the performance of the model trained on the test set - both accuracy score and classification report should be printed.\n","############ Your code goes here ############"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9A2v0TksDMlJ","colab_type":"text"},"source":["## Visualizing The Decision Tree Learnt"]},{"cell_type":"code","metadata":{"id":"XGrX6shf1XC9","colab_type":"code","colab":{}},"source":["import graphviz\n","\n","model = # Your model variable\n","dot_data = tree.export_graphviz(model, out_file=None, \n","                      feature_names=iris.feature_names,  \n","                      class_names=iris.target_names,  \n","                      filled=True, rounded=True,  \n","                      special_characters=True)  \n","graph = graphviz.Source(dot_data)  \n","graph "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5xcixa8Ej3C","colab_type":"text"},"source":["## TO DO: Decision Boundary Visualization\n","\n","Write training code missing in the cell"]},{"cell_type":"code","metadata":{"id":"famthVBJCPVV","colab_type":"code","colab":{}},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","import numpy as np\n","\n","# Hyper-parameters\n","criterion = 'gini'\n","splitter = 'best'\n","max_depth = None\n","min_samples_split = 2\n","min_samples_leaf = 1\n","\n","# Parameters\n","n_classes = 3\n","plot_colors = \"ryb\"\n","plot_step = 0.02\n","plt.figure(figsize=(16, 4))\n","\n","for pairidx, pair in enumerate([[0, 1], [2, 3], [0, 2], [1, 3]]):\n","    # Considers only pairs of features for 2D plotting\n","    X = iris.data[:, pair]\n","    y = iris.target\n","    \n","    # Training Code\n","    ############ Your code goes here ############\n","    \n","    \n","    # Plot the decision boundary\n","    plt.subplot(1, 4, pairidx + 1)\n","    \n","\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                         np.arange(y_min, y_max, plot_step))\n","    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n","\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n","\n","    plt.xlabel(iris.feature_names[pair[0]])\n","    plt.ylabel(iris.feature_names[pair[1]])\n","\n","    # Plot the training points\n","    for i, color in zip(range(n_classes), plot_colors):\n","        idx = np.where(y == i)\n","        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n","                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSTEHwPcH6Sg","colab_type":"text"},"source":["## TO DO: Implement Gini-Index From Scratch"]},{"cell_type":"markdown","metadata":{"id":"DJGwLJlNH6Ve","colab_type":"text"},"source":["In this exercise, you will get your hands dirty and write a method that outputs Gini-index given a split of a toy dataset. "]},{"cell_type":"markdown","metadata":{"id":"Kcy2i2hCH6ay","colab_type":"text"},"source":["### Visualizing The Toy-Data"]},{"cell_type":"code","metadata":{"id":"XGz-UF9R9JBz","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","toy_data = [[4.484812507,3.4983517180,0],\n","            [3.442139098,2.883329202,0],\n","            [5.391887635,4.526381359,0],\n","            [5.674611146,4.333518109,0],\n","            [4.712776711,3.922582001,0],\n","            [9.211113656,4.876521335,1],\n","            [10.71577104,5.052614977,1],\n","            [9.158110115,2.190251164,1],\n","            [11.83850681,4.948118771,1],\n","            [8.355855140,5.033551554,1]]\n","\n","toy_data_pd = pd.DataFrame(toy_data, columns=['x1', 'x2', 'class_id'])\n","plt.scatter(toy_data_pd.x1[toy_data_pd.class_id == 0], toy_data_pd.x2[toy_data_pd.class_id == 0], c='r')\n","plt.scatter(toy_data_pd.x1[toy_data_pd.class_id == 1], toy_data_pd.x2[toy_data_pd.class_id == 1], c='b')\n","plt.xlabel('X1')\n","plt.ylabel('X2')\n","plt.legend(labels=[0,1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdFaTxcEcfHP","colab_type":"text"},"source":["### 1. Method `data_split` takes the dataset and splits it at provided `index` if data less than `value`.  \n","\n","### 2. Method `get_best_split` is a driver method that calclulates `gini_index` on every possible split.\n","\n","### 3. (TO DO) Method `gini_index`, takes `groups` and `classes` info and calculates Gini-index.\n","\n","If a target is a classification outcome taking on values 0,1,…,K-1, for node , representing a region  with observations, let\n","\n",">$p_{mk} = \\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = k)$\n","  \n","be the proportion of class $k$ observations in node $m$.\n","Common measures of impurity are:\n","\n","* Gini\n",">$H(X_m) = \\sum_k p_{mk} (1 - p_{mk})$\n","\n","* Entropy\n",">$H(X_m) = - \\sum_k p_{mk} \\log(p_{mk})$\n","\n","\n","Weighting the impurity across $k$ classes:\n",">$G(Q) = \\frac{n_{left}}{N_m} H(Q_{left})\n","   + \\frac{n_{right}}{N_m} H(Q_{right})$"]},{"cell_type":"code","metadata":{"id":"Rej1srzVvQGN","colab_type":"code","colab":{}},"source":["# A method that does the array split\n","def data_split(index, value, data):\n","  left, right = list(), list()\n","  for row in data:\n","    if row[index] < value:\n","      left.append(row)\n","    else:\n","      right.append(row)\n","  return left, right\n","\n","# Output the best split point for the data\n","def get_best_split(data):\n","  class_values = list(set(row[-1] for row in data))\n","  best_index, best_value, best_score, best_groups = 999, 999, 999, None\n","  for index in range(len(dataset[0])-1):\n","    for row in data:\n","      groups = data_split(index, row[index], data)\n","      gini = gini_index(groups, class_values)\n","      print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n","      if gini < best_score:\n","        best_index, best_value, best_score, best_groups = index, row[index], gini, groups\n","  return {'index':best_index, 'value':best_value, 'groups':best_groups}\\\n","\n","# (TO DO) Calculate the Gini index for a split dataset\n","def gini_index(groups, classes):\n","  ############ Your code goes here ############\n","  \n","  return gini_value\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JF18gfFKxKhn","colab_type":"code","colab":{}},"source":["split = get_best_split(toy_data)\n","print('Best Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WK0Y8PgI_ie","colab_type":"code","colab":{}},"source":["plt.scatter(toy_data_pd.x1[toy_data_pd.class_id == 0], toy_data_pd.x2[toy_data_pd.class_id == 0], c='r')\n","plt.scatter(toy_data_pd.x1[toy_data_pd.class_id == 1], toy_data_pd.x2[toy_data_pd.class_id == 1], c='b')\n","# If best split is on X1, uncomment next line.\n","plt.axvline(x=split['value'])\n","# If best split is on X2, uncomment next line.\n","# plt.axhline(y=split['value'])\n","plt.show()"],"execution_count":0,"outputs":[]}]}